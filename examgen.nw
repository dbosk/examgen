\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}

\usepackage{listings}
\lstset{%
  basicstyle=\footnotesize
}

\usepackage{noweb}
% Needed to relax penalty for breaking code chunks across pages, otherwise 
% there might be a lot of space following a code chunk.
\def\nwendcode{\endtrivlist \endgroup}
\let\nwdocspar=\smallbreak

\usepackage{csquotes}
\MakeBlockQuote{<}{ยง}{>}
\EnableQuotes

\usepackage[natbib,style=alphabetic,backend=bibtexu]{biblatex}
\addbibresource{examgen.bib}

\title{%
  examgen: An exam generator
}
\author{Daniel Bosk}
\affil{%
  School of Computer Science and Communication,\\
  KTH Royal Institute of Technology, SE-100\,44 Stockholm
}
\affil{%
  Department of Information and Communication Systems,\\
  Mid Sweden University, SE-851\,70 Sundsvall
}
\date{Version 3.0}

\begin{document}
\maketitle

\begin{abstract}
  \input{abstract.tex}
\end{abstract}
\clearpage

\tableofcontents
\clearpage

@
\section{Introduction}

The purpose of this program is to automatically generate the LaTeX code for an 
exam based on some inputs.
The inputs will be a database of questions, parameters on how to choose 
questions from the database, meta-data about the exam---e.g.~course name, dates 
etc.

The LaTeX code uses the exam~\cite{exam} document class.
The idea is to have all exam questions available so that examgen(1) can 
randomly select a number of them and put together the [[questions]] environment 
of the exam document class.

We can also let examgen(1) generate the surrounding code, i.e.~setting document 
class, title etc.~given enough input parameters.

\subsection{Outline}

The program is a Python 3 script, [[<<examgen.py>>]].
We will use the following structure:
<<examgen.py>>=
#!/usr/bin/env python3
<<imports>>
<<constants>>
<<classes>>
<<functions>>
def main(argv):
  <<main body>>
if __name__ == "__main__":
  try:
    sys.exit(main(sys.argv))
  except Exception as err:
    print("{0}: {1}".format(sys.argv[0], err), file=sys.stderr)
    sys.exit(1)
@ Then we will successively specify what these mean.
The [[<<imports>>]] will contain our imported modules.
For instance, since we use [[sys.argv]] and [[sys.exit]] above we'll need to 
add
<<imports>>=
import sys
@ to [[<<imports>>]].
The code blocks [[<<classes>>]] and [[<<functions>>]] will contain our classes 
and functions, respectively.

The [[<<main body>>]] block contains the code of the main function.
Basically, we need the following code blocks:
<<main body>>=
<<parse command-line arguments>>
<<generate exam>>
<<output result>>
<<clean up and exit>>
@

To parse command-line arguments we will make use of Python's
[[argparse]]~\cite{argparse}:
<<parse command-line arguments>>=
argp = argparse.ArgumentParser(description="Generates an exam.")
@ We also need to add it to our imports:
<<imports>>=
import argparse
@ The parsing step will then be to [[<<parse arguments>>]] and then
[[<<process arguments>>]]:
<<parse command-line arguments>>=
<<parse arguments>>
<<process arguments>>
@
The processing step is rather straight-forward using [[argparse]].
We simply parse [[argv]] and get a Python dictionary containing the variables 
we specify in [[<<parse arguments>>]]:
<<process arguments>>=
args = vars(argp.parse_args(argv[1:]))
@


\section{Design}

Inevitably, we need to keep the exam questions in some form of database.
Our first option is to reuse the (LaTeX code of) old exams, i.e.~one exam is 
considered one database file.
Thus we need to be able to add several database files, and we must add at least 
one.
We will do that by adding a database argument to the program:
<<parse arguments>>=
argp.add_argument(
  "-d", "--database", nargs="+",
  required=True, help="Adds a questions database to use")
@ This will give us a Python list containing all the names, so we can read all 
questions in each database:
<<process arguments>>=
questions = set()
for database in args["database"]:
  <<read questions database>>
@ We store the questions in a set since we are not interested in any 
redundancy, the same question should not be added twice even if it occurs in 
two database files.

\subsection{The questions database format}

Since the design allows for using old exams as database files, then we must 
adapt our database format to this.
We assume that the exams are using the exam~\cite{exam} document class.
As such each question will start with the command \verb'\question' and end at 
the beginning of the next question.
Or in the special case of the last question, it ends with the end of the 
[[questions]] environment.
However, we also want to be able to use exercises from teaching material.

We can thus make use of Python's regular expressions facilities~\cite{regex}:
<<imports>>=
import re
@ We can use the following code block to set up a regular expression pattern to 
match a question:
<<set up question regex>>=
question_code_pattern = re.compile(
  "(\\\\question(.|\n)*?"
    "(?=(\\\\question|\\\\end{questions}|\\\\begin{exercise}))|"
  "\\\\begin{exercise}(.|\n)*?\\\\end{exercise})",
  re.MULTILINE)
@ (See~\cite{regex-lookaround} for a treatment of zero-width assertions in 
regular expressions.)
The regular expression consists of two parts.
The first part matches questions (exam format) and the second part matches 
exercise environments.
This expression will conveniently also include any parts or solution 
environments used in the exam question format.

To read the questions database we need to do the following:
<<read questions database>>=
<<set up question regex>>
file_contents = open(database, "r").read()
<<match a question>>
while match:
  <<parse question>>
  <<remove matching question>>
  <<match a question>>
@ To match a question we simply let
<<match a question>>=
match = re.search(question_code_pattern, file_contents)
@ This makes [[match]] an object of [[MatchObject]] type.
This means that we can use its [[end()]] method to remove the already searched 
text from [[file_contents]]:
<<remove matching question>>=
file_contents = file_contents[match.end():]
@

\subsection{The question format}

Now that we know the format of the question databases, this brings us to the 
next part: the format of the actual question and how to do
[[<<parse question>>]].
We need some sort of data structure to hold each question and its related 
meta-data.
One solution is to use a class:
<<classes>>=
class Question:
  <<question constructors>>
  <<question methods>>
@

We will have at least one constructor.
A suitable one is to construct the question from its LaTeX code:
<<question constructors>>=
def __init__(self, code):
  self.__code = code
  <<question constructor body>>
@ This makes the LaTeX code also a natural attribute of the class.
We will use Python's properties to do this:
<<question methods>>=
@@property
def code(self):
  return self.__code
@ Now we can let the constructor be
<<question constructor body>>=
if self.__code.find(r"\begin{exercise}") >= 0:
  <<transform exercise to question form>>
@

All questions should be in question format, so for the exercise environments we 
must do some transformation.
To transform an exercise to a question we can replace the beginning of the 
environment with the question command and simply drop the end of the 
environment.
<<transform exercise to question form>>=
self.__code = self.__code.replace(r"\begin{exercise}", r"\question ")
self.__code = self.__code.replace(r"\end{exercise}", "")
@

To be able to add a question to a set, the data structure must be <hashable>:
<<question methods>>=
def __hash__(self):
  return hash(self.code)

def __eq__(self, other):
  if not hasattr(other, "code"):
    return NotImplemented
  return (self.code == other.code)
@ This also means that we are not allowed to modify the question object 
throughout its lifetime, i.e.~the [[code]] attribute must not be modified.

Now that we have everything we need to parse the question, we can thus define
<<parse question>>=
questions.add(Question(match.group()))
@ where we add the question to the set of questions.


\section{Randomly selecting the questions}

% XXX Add references for how to create an exam
The purpose of this work is to construct an exam, hence there are several 
aspects we need to consider.
The exam should examine if the student has reached the intended learning 
outcomes.
Since we cannot examine every detail of the material treated in the course, the 
exam usually depends on a random sample of the material covering the key 
concepts of the course.
Thus, how we select the questions is of great importance.
Firstly we need to identify similar questions, i.e.\ questions covering the 
same things.
This is covered in \cref{sec:tags}.
Secondly we need to make a selection which covers the course in a good way.
In \cref{sec:selection} we describe an algorithm which solves this problem.

\subsection{Tags}
\label{sec:tags}

We have handled the problem of the same question reoccurring.
However, we still have the problem of similar questions occurring.
These questions treat the same topic in a similar way and we must recognize 
them as such: we don't want two questions which are too similar in an exam.
One solution is that we mark the questions, using a system similar to tags.
These tags can be used to identify topics, but also be used to mark the 
difficulty level of the question.
Thus it makes sense to have a set of tags as an attribute for our question 
class:
<<question constructor body>>=
self.__tags = set()
@ Since we cannot modify the question, there is no need to update the tags 
either, so we only need a readable property.
<<question methods>>=
@@property
def tags(self):
  return self.__tags
@ To compute the set of tags for several questions we provide the following 
function.
<<functions>>=
def tags(questions):
  T = set()
  for q in questions:
    T |= q.tags
  return T
@ 

To tag a question, we can use the LaTeX [[\label]] command.
Most questions have an attached label, conventionally prefixed with [[q:]].
We can use this convention to add a colon separated list of tags using the 
label command.
This list can be extracted in the following way:
<<question constructor body>>=
<<question tags regex pattern>>
<<filter out tags using regex>>
@ where
<<question tags regex pattern>>=
question_label_pattern = re.compile(
  "\\\\label{(q|xrc):([^}]*)}",
  re.MULTILINE)
@ (Another possible prefix is [[xrc:]], for exercise.)
We can then extract the tags using the second group in the pattern and add the 
tags to the set of tags:
<<filter out tags using regex>>=
matched_tags = question_label_pattern.finditer(self.code)
for match in matched_tags:
  self.__tags |= set(match.group(2).split(":"))
@ This will yield [[None]] if the regular expression does not match, i.e.\ 
there is no label for the question.
If there is, this will yield a Python list of tags, which is immediately 
converted to a set.

Keeping the tags in the label works if there are a few shortly-named tags, 
however, this approach does not scale well.
For this reason we also want to provide another way to supply several 
longer-named tags, e.g.\ to have intended learning outcomes as tags.
We will do this by a specially crafted comment in the code of the question.
<<question tags regex pattern>>=
question_comment_pattern = re.compile(
  "% ?(tags?|ilos?|ILOs?): ?(.*)$",
  re.MULTILINE)
@ Then we can take the union of these tags and those found in the label:
<<filter out tags using regex>>=
matched_tags = question_comment_pattern.finditer(self.code)
for match in matched_tags:
  self.__tags |= set(match.group(2).split(":"))
@

When we generate an exam, we want to specify which tags we are interested in.
We can do this using an argument on the command-line:
<<parse arguments>>=
argp.add_argument(
  "-t", "--tags", nargs="+",
  required=True, help="Adds required question tags")
@ This will give us a list of tags which we can use when selecting the 
questions for the exam:
<<process arguments>>=
required_tags = set(args["tags"])
@

\subsection{Finding a covering}
\label{sec:selection}

We now have the set of required tags \(E\) for the exam, [[required_tags]] in 
[[<<process arguments>>]] above.
We also have the set of tags \(Q_i\) for each question \(i\), through the 
[[tags]] property in [[<<question methods>>]].
What we want is a set of questions \(\{ Q_i \}_i\) which cover the exam \(E\), 
i.e.~\(E\subseteq \bigcup_i Q_i\).
Since we do not want unnecessarily many questions on the exam, we preferably 
want to find a minimal covering.
Our algorithm will not guarantee a minimal covering, but it will ensure that no 
two questions will cover the same thing.

As implied above, the creation of an exam depends on the required tags and the 
question universe available.
We will actually generalize this a bit, we will use the universe of questions, 
a predicate for stopping and a question filter.
(The reason we include the required tags is to form a more useful error message
in [[<<randomly select a question>>]].)
Thus, to find a covering (generate an exam) we can do the following:
<<functions>>=
def examgen(questions, required_tags, stop_condition, question_filter):
  exam_questions = set()
  while not stop_condition(exam_questions):
    <<randomly select a question>>
    <<check if the question is good>>
    <<add the question to the exam>>
    <<remove the question so we don't select it again>>
  return exam_questions
@ Now we can generate an exam in the following way:
<<generate exam>>=
<<set up stop condition>>
<<set up question filter>>
exam_questions = examgen(questions, required_tags,
  stop_condition, question_filter)
@ We will return to the stop condition in \cref{StopConditions} and filters in, 
first, [[<<check if the question is good>>]] and then \cref{QuestionFilters}.

We can randomly select a question by
<<randomly select a question>>=
try:
  question = random.sample(questions, 1)[0]
except ValueError as err:
  tags_left = required_tags - tags(exam_questions)
  raise ValueError("{0}: missing {1}".format(err, tags_left))
@ [[random.sample]] returns a list of the one sample we requested or it raises 
an exception.

If no error occurred when randomly selecting a question, then we store only 
that sample (instead of a list containing only one sample).
Then we can add the question to the exam by
<<add the question to the exam>>=
exam_questions.add(question)
@ Lastly we remove it from the database so we do not select it again:
<<remove the question so we don't select it again>>=
questions.discard(question)
@ The random selection also requires us to add the [[random]] module:
<<imports>>=
import random
@

The last thing we need to do is to [[<<check if the question is good>>]].
For this purpose we use the [[question_filter]], i.e.\ pass each randomly 
chosen question through the filter to see if it passes.
The filter is allowed to modify the question, so we must handle this case too.
<<check if the question is good>>=
filtered_question = question_filter(question, exam_questions)
if filtered_question is None:
  <<remove the question so we don't select it again>>
  continue
else:
  question = filtered_question
@ Different filters will be treated in \cref{QuestionFilters}.

\subsection{Stop conditions}
\label{StopConditions}

The stop condition suggested above is when the required tags \(E\) is a subset 
of the tags of the questions in the exam, \(E\subseteq \bigcup_i Q_i\).
Now we can formulate that stop condition as:
<<classes>>=
class StopWhenSubset:
  def __init__(self, required_tags):
    self.__req_tags = required_tags

  def __call__(self, selected_questions):
    return self.__req_tags <= tags(selected_questions)
@

Now we can instantiate a stop condition.
<<set up stop condition>>=
stop_condition = StopWhenSubset(required_tags)
@

\subsection{Filtering questions}
\label{QuestionFilters}

The main part of the algorithm is to check if a question is good or not, i.e.\ 
the filtering, and we will cover that part now.
As suggested above, a filter requires two parameters:
the question and the currently selected question.
As a filter we will use a callable object, be it a function or an instance of 
a callable class.
We will discuss a few filters and how to activate them below, however, first we 
will discuss composing filters.

As indicated above, a filter requires two parameters: the question under 
consideration and the already selected questions.
We want to compose such functions, and to do this we can introduce a function 
which returns a new function-like object which is the composed function.
<<functions>>=
def compose_filters(filterA, filterB):
  return lambda x, y: filterB(filterA(x, y), y)
@

\subsubsection{No filtering}

The most basic filter we could use will simply allow all questions to pass.
Sometimes this is actually useful, but it is also good for illustration 
purposes here.
<<classes>>=
class NoFilter:
  def __call__(self, question, selected_questions):
    return question
@ We can add this as the default filter.
<<set up question filter>>=
question_filter = NoFilter()
@

\subsubsection{A question must add new tags}

To decrease the number of questions in an exam we want to require every new 
question to cover new tags, i.e.\ the new question is not a subset of the 
already selected questions.
This way we will not have several questions covering exactly the same tags, but
they are allowed to overlap by a proper subset.
<<classes>>=
class IsNotSubset:
  def __call__(self, question, selected_questions):
    if question is None:
      return None
    elif question.tags <= tags(selected_questions):
      return None
    else:
      return question
@

We will add a command-line option to enable this filter.
<<parse arguments>>=
argp.add_argument(
  "-N", "--require-new-tags",
  default=False, action="store_true",
  help="Requires that each new question adds new tags to the exam")
@ If this option is enabled we will compose this filter with already set 
filters.
<<set up question filter>>=
if args["require_new_tags"]:
  question_filter = \
    compose_filters(question_filter, IsNotSubset())
@

\subsubsection{Questions must have disjoint tag sets}

We can also add a stricter version of the previous filter, i.e.\ to require 
that each question has no overlapping tags.
<<classes>>=
class DisjointQuestions:
  def __call__(self, question, selected_questions):
    if question is None:
      return None
    elif question.tags & tags(selected_questions) != set():
      return None
    else:
      return question
@

We will add the following command-line option to enable this filter.
<<parse arguments>>=
argp.add_argument(
  "-D", "--require-disjoint",
  default=False, action="store_true",
  help="Requires that each question has disjoint tag sets")
@ We will simply compose this filter if enabled.
<<set up question filter>>=
if args["require_disjoint"]:
  question_filter = \
    compose_filters(question_filter, DisjointQuestions())
@

\subsubsection{Any covering}

Now we will introduce the first filter to require that the question actually 
contribute to the covering of the exam's tags.
This mean that a question must have one of the required tags.
<<classes>>=
class RequireCover:
  def __init__(self, required_tags):
    self.__req_tags = required_tags

  def __call__(self, question, selected_questions):
    if question is None:
      return None
    elif question.tags & self.__req_tags != set():
      return question
    else:
      return None
@ This actually prevents questions with no tags.

We provide the following command-line option to enable this filter.
<<parse arguments>>=
argp.add_argument(
  "-C", "--require-cover",
  default=False, action="store_true",
  help="Requires that each question must cover some of the required tags")
@ If this is enabled we will compose the filter.
<<set up question filter>>=
if args["require_cover"]:
  question_filter = \
    compose_filters(question_filter, RequireCover(required_tags))
@

\subsubsection{An exact covering}

Usually we do not want to include questions which cover topics outside the 
scope of the exam.
This requires a stronger filter than the previous one, i.e.\ the question tags 
must be a subset of the exam tags, \(Q_i\subseteq E\).
<<classes>>=
class ExactCovering:
  def __init__(self, required_tags):
    self.__req_tags = required_tags

  def __call__(self, question, selected_questions):
    if question is None:
      return None
    elif question.tags <= self.__req_tags:
      return question
    else:
      return None
@

We will add a command-line option to activate this filter.
<<parse arguments>>=
argp.add_argument(
  "-E", "--require-exact",
  default=False, action="store_true",
  help="Requires every question's tag set to be a subset of the required tags"
    ", i.e. an exact covering")
@ And finally we compose it if enabled.
<<set up question filter>>=
if args["require_exact"]:
  question_filter = \
    compose_filters(question_filter, ExactCovering(required_tags))
@

\subsubsection{Human intervention}

At times we might want some human intervention.
Some of the questions might not be tagged, or not suitably tagged.
Some questions might be good starting points for better questions.
Since the selection is randomized, we might want to have some human 
intervention to guarantee a better selection.
We will do this by opening the question for editing in the user's favourite 
editor.
This will allow the user to edit the question text and its tags.
In essence, the user can now use the exam generator to generate a list of 
questions to use as inspiration for a new exam.

Since this is a feature we might only want occasionally, we add a command-line 
argument to enable it:
<<parse arguments>>=
argp.add_argument(
  "-i", "--interactive",
  default=False, action="store_true",
  help="Turns interactive mode on, "
    "lets you edit each qualifying question with ${EDITOR}")
@ We will create a filter which provides this interaction.
<<classes>>=
class OpenWithEditor:
  def __init__(self, required_tags):
    self.__req_tags = required_tags

  def __call__(self, question, selected_questions):
    if question is None:
      return None
    else:
      return edit_question(question, self.__req_tags, selected_questions)
@ Consequently we will compose this filter in the same fashion as previously.
<<set up question filter>>=
if args["interactive"]:
  question_filter = \
    compose_filters(question_filter, OpenWithEditor(required_tags))
@

The [[edit_question]] function is defined as follows.
<<functions>>=
def edit_question(question, required_tags, exam_questions):
  <<open the question in the editor>>
  <<check that the question is still qualified>>
@

\paragraph{Open the question in the user's editor}

To open the question for editing in the user's editor, we have to write the 
question to a temporary file and open that file with the editor.
Then we read the contents back to process it.
<<open the question in the editor>>=
<<create a temporary file>>
<<execute the editor with file as argument>>
<<read the file contents back>>
@ We will use Python's interface to the operating system to create a temporary 
file in the proper way:
<<create a temporary file>>=
fd, filename = tempfile.mkstemp()
file = os.fdopen(fd, "w")
<<write the question to file>>
file.close()
@ This requires the [[tempfile]] module.
<<imports>>=
import tempfile
@ We open the file by executing what is in the [[EDITOR]] environment variable 
in the shell.
<<execute the editor with file as argument>>=
command = [os.environ.get("EDITOR", "vim"), filename]
subprocess.run(command)
@ This in turn requires more modules:
<<imports>>=
import os, subprocess
@ Finally we open the file when the sub-process (editor) has exited.
When we are done with the file we remove it.
<<read the file contents back>>=
with open(filename, "r") as file:
  <<read the question back from file>>
os.unlink(filename)
@

To aid the user we do not only want to write the question code to the file, we 
also want to include which tags are remaining for a complete cover of the tags.
Thus, first we write the remaining tags followed by a separator and finally the
code of the question.
<<write the question to file>>=
for t in (required_tags - tags(exam_questions)):
  file.write("% remaining tag: " + t + "\n")
file.write("\n" + REMOVE_ABOVE_SEPARATOR + "\n")
file.write(question.code)
@ We add the separator to our constants.
<<constants>>=
REMOVE_ABOVE_SEPARATOR = "% ----- Everything ABOVE will be REMOVED -----"
@ Conversely we also want to read the edited file back when the user is done 
editing.
<<read the question back from file>>=
question_lines = [line.strip("\n") for line in file.readlines()]
try:
  question_lines = \
    question_lines[question_lines.index(REMOVE_ABOVE_SEPARATOR)+1:]
except ValueError:
  pass
finally:
  question = Question("\n".join(question_lines))
@

\paragraph{Check that the question is still qualified}

Since the user is allowed to do arbitrary edits in the question, that means 
that the tags might have changed in such a way that the question is not 
qualified for the exam.
In these circumstances we must inform the user and give the user the chance to 
correct this.
<<check that the question is still qualified>>=
if not question.tags <= required_tags:
  print("{0}: {1}".format(sys.argv[0],
    "the question's tags are not a subset of the required tags"))
  <<ask the user to reject or edit again>>
else:
  <<ask the user to accept, edit again or reject>>
@

Now the user is supposedly done with the question, we should now provide 
alternatives to go back to editing, accepting the edited version or reject it 
and go to the next question.
Or, if the question is no longer qualified, the user must edit or reject.
<<ask the user to accept, edit again or reject>>=
action = input("[e]dit again, [a]ccept, [r]eject: ")
while True:
  if action in {"A", "a", "accept"}:
    return question
  elif action in {"R", "r", "reject"}:
    return None
  elif action in {"E", "e", "edit"}:
    return edit_question(question, required_tags, exam_questions)
  else:
    action = input("[e]dit again, [a]ccept, [r]eject: ")
@ Similarly we get
<<ask the user to reject or edit again>>=
action = input("[e]dit again, [r]eject: ")
while True:
  if action in {"R", "r", "reject"}:
    return None
  elif action in {"E", "e", "edit"}:
    return edit_question(question, required_tags, exam_questions)
  else:
    action = input("[e]dit again, [r]eject: ")
@


\section{Generating the output}

Now that we have all the questions in [[exam_questions]] (see
[[<<generate exam>>]] above), we can move on to the problem of outputting it.
So in this section we define [[<<output result>>]].

The trivial solution is to just print the code for the questions to standard 
out.
Hence we let
<<output result>>=
for q in exam_questions:
  print("%s" % q.code)
@


\section*{Acknowledgements}

\input{acknowledgements.tex}


\printbibliography


\section*{An index of the code blocks}

\nowebchunks


\appendix
\section{An example exam}
\label{sec:ExampleExam}
\lstinputlisting[numbers=left,language={[latex]tex}]{dasak-150604.tex}


\end{document}
