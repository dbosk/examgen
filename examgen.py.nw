\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}

\usepackage{listings}
\lstset{%
  basicstyle=\footnotesize
}

\usepackage{noweb}
% Needed to relax penalty for breaking code chunks across pages, otherwise 
% there might be a lot of space following a code chunk.
\def\nwendcode{\endtrivlist \endgroup}
\let\nwdocspar=\smallbreak

\usepackage{csquotes}
\MakeBlockQuote{<}{ยง}{>}
\EnableQuotes

\usepackage[natbib,style=alphabetic,backend=bibtexu]{biblatex}
\addbibresource{examgen.bib}

\title{%
  examgen: An Exam Generator
}
\author{Daniel Bosk}
\affil{%
  School of Computer Science and Communication,\\
  KTH Royal Institute of Technology, SE-100\,44 Stockholm
}
\affil{%
  Department of Information and Communication Systems,\\
  Mid Sweden University, SE-851\,70 Sundsvall
}
\date{Version 2.0}

\begin{document}
\maketitle

\begin{abstract}
  \input{abstract.tex}
\end{abstract}
\clearpage

\tableofcontents
\clearpage

@
\section{Introduction}

The purpose of this program is to automatically generate the LaTeX code for an 
exam based on some inputs.
The inputs will be a database of questions, parameters on how to choose 
questions from the database, meta-data about the exam---e.g.~course name, dates 
etc.

The LaTeX code uses the exam~\cite{exam} document class.
The idea is to have all exam questions available so that examgen(1) can 
randomly select a number of them and put together the [[questions]] environment 
of the exam document class.

We can also let examgen(1) generate the surrounding code, i.e.~setting document 
class, title etc.~given enough input parameters.

\subsection{Outline}

The program is a Python 3 script, [[<<examgen.py>>]].
We will use the following structure:
<<examgen.py>>=
#!/usr/bin/env python3
<<imports>>
<<constants>>
<<classes>>
<<functions>>
def main(argv):
  <<main body>>
if __name__ == "__main__":
  try:
    sys.exit(main(sys.argv))
  except Exception as err:
    print("{0}: {1}".format(sys.argv[0], err), file=sys.stderr)
    sys.exit(1)
@ Then we will successively specify what these mean.
The [[<<imports>>]] will contain our imported modules.
For instance, since we use [[sys.argv]] and [[sys.exit]] above we'll need to 
add
<<imports>>=
import sys
@ to [[<<imports>>]].
The code blocks [[<<classes>>]] and [[<<functions>>]] will contain our classes 
and functions, respectively.

The [[<<main body>>]] block contains the code of the main function.
Basically, we need the following code blocks:
<<main body>>=
<<parse command-line arguments>>
<<generate exam>>
<<output result>>
<<clean up and exit>>
@

To parse command-line arguments we will make use of Python's
[[argparse]]~\cite{argparse}:
<<parse command-line arguments>>=
argp = argparse.ArgumentParser(description="Generates an exam.")
@ We also need to add it to our imports:
<<imports>>=
import argparse
@ The parsing step will then be to [[<<parse arguments>>]] and then
[[<<process arguments>>]]:
<<parse command-line arguments>>=
<<parse arguments>>
<<process arguments>>
@
The processing step is rather straight-forward using [[argparse]].
We simply parse [[argv]] and get a Python dictionary containing the variables 
we specify in [[<<parse arguments>>]]:
<<process arguments>>=
args = vars(argp.parse_args(argv[1:]))
@


\section{Design}

Inevitably, we need to keep the exam questions in some form of database.
Our first option is to reuse the (LaTeX code of) old exams, i.e.~one exam is 
considered one database file.
Thus we need to be able to add several database files, and we must add at least 
one.
We will do that by adding a database argument to the program:
<<parse arguments>>=
argp.add_argument(
  "-d", "--database", nargs="+",
  required=True, help="Adds a questions database to use")
@ This will give us a Python list containing all the names, so we can read all 
questions in each database:
<<process arguments>>=
questions = set()
for database in args["database"]:
  <<read questions database>>
@ We store the questions in a set since we are not interested in any 
redundancy, the same question should not be added twice even if it occurs in 
two database files.

\subsection{The Questions Database Format}

Since the design allows for using old exams as database files, then we must 
adapt our database format to this.
We assume that the exams are using the exam~\cite{exam} document class.
As such each question will start with the command \verb'\question' and end at 
the beginning of the next question.
Or in the special case of the last question, it ends with the end of the 
[[questions]] environment.
However, we also want to be able to use exercises from teaching material.

We can thus make use of Python's regular expressions facilities~\cite{regex}:
<<imports>>=
import re
@ We can use the following code block to set up a regular expression pattern to 
match a question:
<<set up question regex>>=
question_code_pattern = re.compile(
  "(\\\\question(.|\n)*?"
    "(?=(\\\\question|\\\\end{questions}|\\\\begin{exercise}))|"
  "\\\\begin{exercise}(.|\n)*?\\\\end{exercise})",
  re.MULTILINE)
@ (See~\cite{regex-lookaround} for a treatment of zero-width assertions in 
regular expressions.)
The regular expression consists of two parts.
The first part matches questions (exam format) and the second part matches 
exercise environments.
This expression will conveniently also include any parts or solution 
environments used in the exam question format.

To read the questions database we need to do the following:
<<read questions database>>=
<<set up question regex>>
file_contents = open(database, "r").read()
<<match a question>>
while match:
  <<parse question>>
  <<remove matching question>>
  <<match a question>>
@ To match a question we simply let
<<match a question>>=
match = re.search(question_code_pattern, file_contents)
@ This makes [[match]] an object of [[MatchObject]] type.
This means that we can use its [[end()]] method to remove the already searched 
text from [[file_contents]]:
<<remove matching question>>=
file_contents = file_contents[match.end():]
@

\subsection{The Question Format}

Now that we know the format of the question databases, this brings us to the 
next part: the format of the actual question and how to do
[[<<parse question>>]].
We need some sort of data structure to hold each question and its related 
meta-data.
One solution is to use a class:
<<classes>>=
class Question:
  <<question constructors>>
  <<question methods>>
@

We will have at least one constructor.
A suitable one is to construct the question from its LaTeX code:
<<question constructors>>=
def __init__(self, code):
  <<question constructor body>>
@ This LaTeX code is also a natural attribute of the class.
However, it should be on the exam question format, so for the exercise 
environments we must do some transformation.
<<question constructor body>>=
self.__code = code
if self.__code.find(r"\begin{exercise}") >= 0:
  <<transform exercise to question form>>
@ We also need a get-method for the code attribute:
<<question methods>>=
def get_code(self):
  return self.__code
@

To transform an exercise to a question we can replace the beginning of the 
environment with the question command and simply drop the end of the 
environment.
<<transform exercise to question form>>=
self.__code = self.__code.replace(r"\begin{exercise}", r"\question ")
self.__code = self.__code.replace(r"\end{exercise}", "")
@

To be able to add a question to a set, the data structure must be <hashable>:
<<question methods>>=
def __hash__(self):
  return hash(self.__code)

def __eq__(self, other):
  if not hasattr(other, "code"):
    return NotImplemented
  return (self.__code == other.__code)
@ This also means that we are not allowed to modify the question object 
throughout its lifetime, i.e.~the [[code]] attribute must not be modified.

Now that we have everything we need to parse the question, we can thus define
<<parse question>>=
questions.add(Question(match.group()))
@ where we add the question to the set of questions.


\section{Randomly Selecting the Questions}

% XXX Add references for how to create an exam
The purpose of this work is to construct an exam, hence there are several 
aspects we need to consider.
The exam should examine if the student has reached the intended learning 
outcomes.
Since we cannot examine every detail of the material treated in the course, the 
exam usually depends on a random sample of the material covering the key 
concepts of the course.
Thus, how we select the questions is of great importance.
Firstly we need to identify similar questions.
This is covered in \cref{sec:tags}.
Secondly we need to make a selection which covers the course in a good way, 
both in topics and in difficulty.
In \cref{sec:selection} we describe an algorithm which solves this problem.

\subsection{Tags}
\label{sec:tags}

We have handled the problem of the same question reoccurring.
However, we still have the problem of similar questions occurring.
These questions treat the same topic in a similar way and we must recognize 
them as such: we don't want two questions which are too similar in an exam.
One solution is that we mark the questions, using a system similar to tags.
These tags can be used to identify topics, but also be used to mark the 
difficulty level of the question.
Thus it makes sense to have a set of tags as an attribute for our question 
class:
<<question constructor body>>=
self.__tags = set()
@ We also need a method to get this set:
<<question methods>>=
def get_tags(self):
  return self.__tags
@

To tag a question, we can use the LaTeX [[\label]] command.
Most questions have an attached label, conventionally prefixed with [[q:]].
We can use this convention to add a colon separated list of tags using the 
label command.
(See line 87 in \cref{sec:ExampleExam} for an example.)
This list can be extracted in the following way:
<<question constructor body>>=
self.__tags = set()
<<question tags regex pattern>>
<<filter out tags using regex>>
@ where
<<question tags regex pattern>>=
question_label_pattern = re.compile(
  "\\\\label{(q|xrc):([^}]*)}",
  re.MULTILINE)
@ (Another possible prefix is [[xrc:]], for exercise.)
We can then extract the tags using the second group in the pattern and add the 
tags to the set of tags:
<<filter out tags using regex>>=
matched_tags = question_label_pattern.finditer(self.__code)
for match in matched_tags:
  self.__tags |= set(match.group(2).split(":"))
@ This will yield [[None]] if the regular expression does not match, i.e.\ 
there is no label for the question.
If there is, this will yield a Python list of tags, which is immediately 
converted to a set.

Keeping the tags in the label works if there are a few shortly-named tags, 
however, this approach does not scale well.
For this reason we also want to provide another way to supply several 
longer-named tags, e.g.\ to have intended learning outcomes as tags.
We will do this by a specially crafted comment in the code of the question.
<<question tags regex pattern>>=
question_comment_pattern = re.compile(
  "% ?(tags?|ilos?|ILOs?): ?(.*)$",
  re.MULTILINE)
@ Then we can take the union of these tags and those found in the label:
<<filter out tags using regex>>=
matched_tags = question_comment_pattern.finditer(self.__code)
for match in matched_tags:
  self.__tags |= set(match.group(2).split(":"))
@

When we generate an exam, we want to specify which tags we are interested in.
We can do this using an argument on the command-line:
<<parse arguments>>=
argp.add_argument(
  "-t", "--tags", nargs="+",
  required=True, help="Adds required question tags")
@ This will give us a list of tags which we can use when selecting the 
questions for the exam:
<<process arguments>>=
required_tags = set(args["tags"])
@

\subsection{Handling Topics, Difficulty Levels or Intended Learning Outcomes}
\label{sec:selection}

We now have the set of required tags \(E\) for the exam, [[required_tags]] in 
[[<<process arguments>>]] above.
We also have the set of tags \(Q_i\) for each question \(i\), through the 
[[get_tags()]] method in [[<<question methods>>]].
What we want is a set of questions \(\{ Q_i \}_i\) which cover the exam \(E\), 
i.e.~\(E = \bigcup_i Q_i\).
Since we do not want unnecessarily many questions on the exam, we preferably 
want to find a minimal cover.
However, our algorithm will not guarantee a minimal covering.

We can add topics, difficulty levels and intended learning outcomes as required 
tags for the exam.
Thus, with the suggested approach we will cover topics, all necessary 
difficulty levels and all intended learning outcomes.

\subsubsection{Finding a Covering}

To find the covering of the exam \(E\), we will use the following approach:
<<generate exam>>=
exam_questions = set()
<<until the exam is covered>>:
  <<randomly select a question>>
  <<check if the question is good>>
  <<add the question to the exam>>
  <<remove the question so we don't select it again>>
@

Our stop condition requires us to find the set of tags of the currently chosen 
questions (\(\{Q_i\}_i = [[exam_questions]]\)).
We can use the following function to find \(\bigcup_i Q_i\):
<<functions>>=
def tags(questions):
  T = set()
  for q in questions:
    T |= q.get_tags()
  return T
@ Thus our stop condition can be formulated as follows:
<<until the exam is covered>>=
while required_tags != tags(exam_questions)
@

We can randomly select a question by
<<randomly select a question>>=
try:
  question = random.sample(questions, 1)[0]
except ValueError as err:
  tags_left = required_tags - tags(exam_questions)
  raise ValueError("{0}: {1}".format(err, tags_left))
@ [[random.sample]] returns a list of the one sample we requested or it raises 
an exception.
In case of an exception, we want to make the error message more useful:
thus we add the list of tags left when the error occurred.
If no error occurred, then we store only that sample instead of the list.
Then we can add the question to the exam by
<<add the question to the exam>>=
exam_questions.add(question)
@ Lastly we remove it from the database so we do not select it again:
<<remove the question so we don't select it again>>=
questions.discard(question)
@ The random selection also requires us to add the [[random]] module:
<<imports>>=
import random
@

\subsubsection{Checking if a Question is Good}

The main part of the algorithm is [[<<check if the question is good>>]], and we 
will cover that part now.
We do not want to include questions which cover topics outside the scope of the 
exam, hence the question tags must be a subset of the exam tags, 
i.e.~\(Q_i\subset E\).
However, sometimes we want to check this manually.
Thus we let
<<check if the question is good>>=
if not args["manual"] and not question.get_tags() <= required_tags:
  <<remove the question so we don't select it again>>
  continue
@ We add a command-line option for fully manual mode:
<<parse arguments>>=
argp.add_argument(
  "-m", "--manual", default=False, action="store_true",
  help="Sets manual mode, "
    "disables filtering algorithm so all questions qualify")
@

We also want to guarantee that the algorithm progresses towards the stop 
condition.
To accomplish this we require that the new question \(Q_j\) is not a subset of 
the tags for all already selected questions, \(\bigcup_i Q_i\).
However, we have a special case: if we use an old exam which is not properly 
tagged, then we have questions with an empty tag set.
We must take these into account as well: these questions will always be 
a subset as the empty set is a subset of any set.
Thus we let
<<check if the question is good>>=
elif len(question.get_tags()) > 0 and \
    question.get_tags() <= tags(exam_questions):
  <<remove the question so we don't select it again>>
  continue
@

\subsection{Human Intervention}

At times we might want some human intervention.
Some of the questions might not be tagged, or not suitably tagged.
Some questions might be good starting points for better questions.
Since the selection is randomized, we might want to have some human 
intervention to guarantee a better selection.
We will do this by opening the question for editing in the user's favourite 
editor.
This will allow the user to edit the question text and its tags.
In essence, the user can now use the exam generator to generate a list of 
questions to use as inspiration for a new exam.

Since this is a feature we might only want occasionally, we add a command-line 
argument to enable it:
<<parse arguments>>=
argp.add_argument(
  "-i", "--interactive",
  default=False, action="store_true",
  help="Turns interactive mode on, "
    "lets you edit each qualifying question with ${EDITOR}")
@ We can add the interaction when we check the question in
[[<<check if the question is good>>]].
We call a function which returns a possibly new version of the question if the 
user accepts it, or [[None]] if the user rejects the question.
<<check if the question is good>>=
elif args["interactive"]:
  new_question = edit_question(question, required_tags, exam_questions)
  if new_question is not None:
    <<remove the question so we don't select it again>>
    question = new_question
  else:
    <<remove the question so we don't select it again>>
    continue
@ The [[edit_question]] function is defined as follows.
<<functions>>=
def edit_question(question, required_tags, exam_questions):
  <<open the question in the editor>>
  <<ask the user to accept, edit again or reject>>
@

\subsubsection{Open the Question in the User's Editor}

To open the question for editing in the user's editor, we have to write the 
question to a temporary file and open that file with the editor.
Then we read the contents back to process it.
<<open the question in the editor>>=
<<create a temporary file>>
<<execute the editor with file as argument>>
<<read the file contents back>>
@ We will use Python's interface to the operating system to create a temporary 
file in the proper way:
<<create a temporary file>>=
fd, filename = tempfile.mkstemp()
file = os.fdopen(fd, "w")
<<write the question to file>>
file.close()
@ This requires the [[tempfile]] module.
<<imports>>=
import tempfile
@ We open the file by executing what is in the [[EDITOR]] environment variable 
in the shell.
<<execute the editor with file as argument>>=
command = [os.environ.get("EDITOR", "vim"), filename]
subprocess.run(command)
@ This in turn requires more modules:
<<imports>>=
import os, subprocess
@ Finally we open the file when the sub-process (editor) has exited.
When we are done with the file we remove it.
<<read the file contents back>>=
with open(filename, "r") as file:
  <<read the question back from file>>
os.unlink(filename)
@

To aid the user we do not only want to write the question code to the file, we 
also want to include which tags are remaining for a complete cover of the tags.
Thus, first we write the remaining tags followed by a separator and finally the
code of the question.
<<write the question to file>>=
for t in (required_tags - tags(exam_questions)):
  file.write("% remaining tag: " + t + "\n")
file.write("\n" + REMOVE_ABOVE_SEPARATOR + "\n")
file.write(question.get_code())
@ We add the separator to our constants.
<<constants>>=
REMOVE_ABOVE_SEPARATOR = "% ----- Everything ABOVE will be REMOVED -----"
@ Conversely we also want to read the edited file back when the user is done 
editing.
<<read the question back from file>>=
question_lines = [line.strip("\n") for line in file.readlines()]
try:
  question_lines = \
    question_lines[question_lines.index(REMOVE_ABOVE_SEPARATOR)+1:]
except ValueError:
  pass
finally:
  question = Question("\n".join(question_lines))
@

\subsubsection{Accept, Reject or Edit Again}

Now the user is supposedly done with the question, we should now provide 
alternatives to go back to editing, accepting the edited version or reject it 
and go to the next question.
<<ask the user to accept, edit again or reject>>=
action = input("[e]dit again, [a]ccept, [r]eject: ")
while True:
  if action in {"A", "a"}:
    return question
  elif action in {"R", "r"}:
    return None
  elif action in {"E", "e"}:
    <<open the question in the editor>>
  action = input("[e]dit again, [a]ccept, [r]eject: ")
@


\section{Generating the Output}

Now that we have all the questions in [[exam_questions]] (see
[[<<generate exam>>]] above), we can move on to the problem of outputting it.
So in this section we define [[<<output result>>]].

The trivial solution is to just print the code for the questions to standard 
out.
Hence we let
<<output result>>=
for q in exam_questions:
  print("%s" % q.get_code())
@


\section*{Acknowledgements}

\input{acknowledgements.tex}


\printbibliography


\section*{An Index of the Code Blocks}

\nowebchunks


\appendix
\section{An Example Exam}
\label{sec:ExampleExam}
\lstinputlisting[numbers=left,language={[latex]tex}]{dasak-150604.tex}


\end{document}
